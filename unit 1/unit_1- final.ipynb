{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.א\n",
    "import openpyxl\n",
    "import re\n",
    "\n",
    "def read_xlsx_in_chunks(file_path, chunk_size=10000):\n",
    "    \"\"\"\n",
    "    קורא קובץ XLSX בחלקים באמצעות גנרטור.\n",
    "    \n",
    "    \"\"\"\n",
    "    workbook = openpyxl.load_workbook(file_path, read_only=True)\n",
    "    sheet = workbook.active\n",
    "    chunk = []\n",
    "    for row in sheet.rows:\n",
    "        chunk.append(row[0].value)  \n",
    "        if len(chunk) == chunk_size:\n",
    "            yield chunk\n",
    "            chunk = []\n",
    "    if chunk:\n",
    "        yield chunk\n",
    "    workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log_file_in_chunks(file_path, chunk_size=10000):\n",
    "    \"\"\"\n",
    "    קורא קובץ לוג txt בחלקים באמצעות גנרטור.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        chunk = []\n",
    "        for line in f:\n",
    "            chunk.append(line.strip())  \n",
    "            if len(chunk) == chunk_size:\n",
    "                yield chunk\n",
    "                chunk = []\n",
    "        if chunk:\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_error_codes(log_line):\n",
    "    \"\"\"\n",
    "    מחפש אתהשגיאה באצעות regex.\n",
    "    \"\"\"\n",
    "    match = re.search(r'Error: (\\w+_\\d+)', log_line)\n",
    "    return match.group(1) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def process_chunk(chunk, results):\n",
    "    \"\"\"\n",
    "    מעבד חלק של נתונים וסופר את קודי השגיאה.\n",
    "    \"\"\"\n",
    "    error_counts = Counter()\n",
    "    for log_line in chunk:\n",
    "        error_code = extract_error_codes(log_line)\n",
    "        if error_code:\n",
    "            error_counts[error_code] += 1\n",
    "    results.append(error_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "def count_errors_parallel(file_path, num_threads=4):\n",
    "    \"\"\"\n",
    "    סופר קודי שגיאות במקביל באמצעות threads.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    threads = []\n",
    "    for chunk in read_xlsx_in_chunks(file_path):\n",
    "    # for chunk in read_log_file_in_chunks(file_path): #--- עבור קובץ txt\n",
    "        thread = threading.Thread(target=process_chunk, args=(chunk, results))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    merged_counts = Counter()\n",
    "    for error_counts in results:\n",
    "        merged_counts.update(error_counts)\n",
    "    return merged_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def find_top_n_errors(error_counts, n):\n",
    "    \"\"\"\n",
    "    מוצא את N קודי השגיאה השכיחים ביותר.\n",
    "    :param error_counts: מילון ספירות שגיאות.\n",
    "    :param n: מספר קודי השגיאה השכיחים ביותר למצוא.\n",
    "    :return: רשימה של N קודי השגיאה השכיחים ביותר וספירותיהם.\n",
    "    \"\"\"\n",
    "    return heapq.nlargest(n, error_counts.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 error codes:\n",
      "WARN_101: 200098\n",
      "ERR_404: 200094\n",
      "ERR_400: 200069\n",
      "INFO_200: 199931\n",
      "ERR_500: 199808\n"
     ]
    }
   ],
   "source": [
    "file_path = './logs.txt.xlsx' \n",
    "n = 10 \n",
    "\n",
    "error_counts = count_errors_parallel(file_path)\n",
    "top_n_errors = find_top_n_errors(error_counts, n)\n",
    "\n",
    "print(f'Top {n} error codes:')\n",
    "for error_code, count in top_n_errors:\n",
    "    print(f'{error_code}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. סיבוכיות זמן:\n",
    "\n",
    "# O(n + n log k) \n",
    "# O(n): קריאת הקובץ - בסוף הרי עוברים על כל השורות. \n",
    "# O(n log k): מיון וחיפוש N השגיאות הנפוצות ביותר (k הוא מספר קודי השגיאה השונים). \n",
    "# השימוש ב Threads מקביל את התהליך ולכן מקצר את זמן הריצה, אך הסיבוכיות האסימפטוטית לא משתנה. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  2. סיבוכיות זיכרון:\n",
    "\n",
    "# O(chunk_size + k)\n",
    "# O(chunk_size = 100000): אחסון חלקי הנתונים בזיכרון.\n",
    "# O(k): אחסון ספירות השגיאות (k הוא מספר קודי השגיאה השונים).\n",
    "# זכרון נשמר במקטעים, לכן הסיבוכיות תלויה בגודל המקטע. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "מתחיל בדיקות נתונים...\n",
      "בדיקה: פורמט התאריך תקין.\n",
      "אזהרה: נמצאו 88911 שורות כפולות.\n",
      "אזהרה: נמצאו ערכים חסרים:\n",
      "timestamp        0\n",
      "value        99040\n",
      "dtype: int64\n",
      "שגיאה: עמודת 'value' חייבת להכיל נתונים מספריים.\n"
     ]
    }
   ],
   "source": [
    "# 1. ב.1.א\n",
    "import pandas as pd\n",
    "\n",
    "def perform_data_checks(df):\n",
    "    print(\"מתחיל בדיקות נתונים...\")\n",
    "\n",
    "    try:\n",
    "        pd.to_datetime(df['timestamp'], format='%d/%m/%Y %H:%M')\n",
    "        print(\"בדיקה: פורמט התאריך תקין.\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"שגיאה: פורמט התאריך בעמודת 'timestamp' אינו תקין.\")\n",
    "\n",
    "    duplicates = df.duplicated()\n",
    "    if duplicates.any():\n",
    "        print(f\"אזהרה: נמצאו {duplicates.sum()} שורות כפולות.\")\n",
    "        # אפשרות להסיר כפילויות:\n",
    "        # df = df.drop_duplicates(inplace=True)\n",
    "    else:\n",
    "        print(\"בדיקה: לא נמצאו כפילויות.\")\n",
    "\n",
    "    # בדיקה של ערכים חסרים\n",
    "    if df.isnull().any().any():\n",
    "        print(\"אזהרה: נמצאו ערכים חסרים:\")\n",
    "        print(df.isnull().sum())\n",
    "        # אפשרות לטפל בערכים חסרים (ע\"י הסרה)\n",
    "        # df.dropna(inplace=True)\n",
    "    else:\n",
    "        print(\"בדיקה: לא נמצאו ערכים חסרים.\")\n",
    "\n",
    "    if not pd.api.types.is_numeric_dtype(df['value']):\n",
    "        raise TypeError(\"שגיאה: עמודת 'value' חייבת להכיל נתונים מספריים.\")\n",
    "    else:\n",
    "        print(\"בדיקה: סוג הנתונים בעמודת 'value' תקין.\")\n",
    "\n",
    "    print(\"בדיקות נתונים הסתיימו.\")\n",
    "    return df\n",
    "\n",
    "try:\n",
    "    time_series_df = pd.read_csv('time_series.csv')\n",
    "    time_series_df = perform_data_checks(time_series_df)\n",
    "    print(\"\\nDataFrame לאחר בדיקות:\")\n",
    "    print(time_series_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"שגיאה: קובץ time_series.csv לא נמצא.\")\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hourly averages saved to 'hourly_averages.csv'\n",
      "   mean_value           time_start\n",
      "0   50.562894  2025-06-01 00:00:00\n",
      "1   49.939803  2025-06-01 01:00:00\n",
      "2   49.457213  2025-06-01 02:00:00\n",
      "3   50.181573  2025-06-01 03:00:00\n",
      "4   48.611496  2025-06-01 04:00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def calc_hourly_avg(file, output_file):\n",
    "    df = pd.read_csv(file, parse_dates=['timestamp'], dayfirst=True)\n",
    "    df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "    df['hour'] = df['timestamp'].dt.floor('h')\n",
    "    hour_avg = df.groupby('hour')['value'].mean().reset_index()\n",
    "    hour_avg['hour'] = hour_avg['hour'].apply(lambda x: x.strftime('%Y-%m-%d %H:00:00'))\n",
    "    hour_avg.rename(columns={'hour': 'time_start', 'value': 'mean_value'}, inplace=True)\n",
    "    hour_avg = hour_avg[['mean_value', 'time_start']]\n",
    "    hour_avg.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(f\"Hourly averages saved to '{output_file}'\")\n",
    "\n",
    "file = 'time_series.csv'\n",
    "hourly_avg_file = 'hourly_averages.csv'\n",
    "calc_hourly_avg(file, hourly_avg_file)\n",
    "hourly_avg = pd.read_csv(hourly_avg_file)\n",
    "print(hourly_avg.head())\n",
    "\n",
    "# סיבוכיות זמן - O(m log h)\n",
    "# כי עובר על כל השורות ומקבץ לפי השעות\n",
    "\n",
    "# סיבוכיות מקום - O(m)\n",
    "# כי שומר את כל השורות בזיכרון\n",
    "# סיבוכיות מקום לקובץ של השעות- O(h)\n",
    "# כי שומר לפי מספר השעות בזיכרון"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hourly averages (processed daily) saved to 'hourly_averages_1.csv'\n",
      "   mean_value           time_start\n",
      "0   50.562894  2025-06-01 00:00:00\n",
      "1   49.939803  2025-06-01 01:00:00\n",
      "2   49.457213  2025-06-01 02:00:00\n",
      "3   50.181573  2025-06-01 03:00:00\n",
      "4   48.611496  2025-06-01 04:00:00\n"
     ]
    }
   ],
   "source": [
    "# 1.  ב.1.ב\n",
    "# חלוקה לפי ימים מבחינה לוגית\n",
    "\n",
    "def calculate_hourly_average_parts(file_path, output_file):\n",
    "    all_hourly_averages = []\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, parse_dates=['timestamp'], dayfirst=True)\n",
    "        df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "        df['date'] = df['timestamp'].dt.date\n",
    "        unique_dates = sorted(df['date'].unique())\n",
    "\n",
    "        for date in unique_dates:\n",
    "            daily_data = df[df['date'] == date].copy()\n",
    "            daily_data['hour'] = daily_data['timestamp'].dt.floor('h')\n",
    "            hourly_avg = daily_data.groupby('hour')['value'].mean().reset_index()\n",
    "            hourly_avg['hour'] = hourly_avg['hour'].apply(lambda x: x.strftime('%Y-%m-%d %H:00:00'))\n",
    "            hourly_avg.rename(columns={'hour': 'time_start', 'value': 'mean_value'}, inplace=True)\n",
    "            all_hourly_averages.append(hourly_avg[['mean_value', 'time_start']])\n",
    "\n",
    "        if all_hourly_averages:\n",
    "            final_hourly_averages = pd.concat(all_hourly_averages).sort_values(by='time_start').reset_index(drop=True)\n",
    "            final_hourly_averages.to_csv(output_file, index=False, encoding='utf-8')\n",
    "            print(f\"Hourly averages (processed daily) saved to '{output_file}'\")\n",
    "        else:\n",
    "            print(\"לא נמצאו נתונים לעיבוד.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"שגיאה: קובץ לא נמצא: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"שגיאה כללית: {e}\")\n",
    "\n",
    "file = 'time_series.csv'\n",
    "output_file_simple = 'hourly_averages_1.csv'\n",
    "calculate_hourly_average_parts(file, output_file_simple)\n",
    "\n",
    "hourly_avg_simple = pd.read_csv(output_file_simple)\n",
    "print(hourly_avg_simple.head())\n",
    "\n",
    "\n",
    "# סיבוכיות זמן - O(m log h)\n",
    "# כי עובר על כל השורות ומקבץ לפי השעות\n",
    "\n",
    "# סיבוכיות מקום - O(m)\n",
    "# כי טוען בתחילה את כל הקובץ\n",
    "# מעבד כל פעם רק O(h_day)\n",
    "# לכל יום את כמות השעות באותו יום\n",
    "# שומר בסוף קובץ בגודל O(h) כמות השעות"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-01.csv\n",
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-02.csv\n",
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-03.csv\n",
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-04.csv\n",
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-05.csv\n",
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-06.csv\n",
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-07.csv\n",
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-08.csv\n",
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-09.csv\n",
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-10.csv\n",
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-11.csv\n",
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-12.csv\n",
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-13.csv\n",
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n",
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-15.csv\n",
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n",
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-17.csv\n",
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n",
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-20.csv\n",
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-21.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n",
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-22.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-23.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-24.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-25.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-26.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-27.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-28.csv\n",
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-29.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n",
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "קובץ יומי נוצר: daily_data_sorted\\daily_data_2025-06-30.csv\n",
      "ממוצעים שעתיים נשמרו ב: hourly_averages_2.csv\n",
      "   mean_value           time_start\n",
      "0   50.562894  2025-01-06 00:00:00\n",
      "1   49.939803  2025-01-06 01:00:00\n",
      "2   49.457213  2025-01-06 02:00:00\n",
      "3   50.181573  2025-01-06 03:00:00\n",
      "4   48.611496  2025-01-06 04:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_39384\\1602750990.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n"
     ]
    }
   ],
   "source": [
    "# חלוקה פיזית\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def calculate_hourly_average(input_file, output_dir, final_output_file):\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        df = pd.read_csv(input_file, parse_dates=['timestamp'], dayfirst=True)\n",
    "        df.sort_values(by='timestamp', inplace=True) \n",
    "\n",
    "        grouped = df.groupby(df['timestamp'].dt.date)\n",
    "\n",
    "        all_hourly_averages = []\n",
    "        for date, daily_data in grouped:\n",
    "            daily_file = os.path.join(output_dir, f\"daily_data_{date}.csv\")\n",
    "            daily_data[['timestamp', 'value']].to_csv(daily_file, index=False, encoding='utf-8')\n",
    "            print(f\"קובץ יומי נוצר: {daily_file}\")\n",
    "\n",
    "            daily_df = pd.read_csv(daily_file, parse_dates=['timestamp'], dayfirst=True)\n",
    "            daily_df['value'] = pd.to_numeric(daily_df['value'], errors='coerce')\n",
    "            daily_df['hour'] = daily_df['timestamp'].dt.floor('h')\n",
    "            hourly_avg = daily_df.groupby('hour')['value'].mean().reset_index()\n",
    "            hourly_avg['hour'] = hourly_avg['hour'].apply(lambda x: x.strftime('%Y-%m-%d %H:00:00'))\n",
    "            hourly_avg.rename(columns={'hour': 'time_start', 'value': 'mean_value'}, inplace=True)\n",
    "            all_hourly_averages.append(hourly_avg[['mean_value', 'time_start']])\n",
    "            os.remove(daily_file)\n",
    "\n",
    "        if all_hourly_averages:\n",
    "            final_hourly_averages = pd.concat(all_hourly_averages).sort_values(by='time_start').reset_index(drop=True)\n",
    "            final_hourly_averages.to_csv(final_output_file, index=False, encoding='utf-8')\n",
    "            print(f\"ממוצעים שעתיים נשמרו ב: {final_output_file}\")\n",
    "        else:\n",
    "            print(\"לא נמצאו נתונים לעיבוד.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"שגיאה: קובץ לא נמצא: {input_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"שגיאה כללית: {e}\")\n",
    "\n",
    "input_file = 'time_series.csv'\n",
    "output_directory = 'daily_data_sorted'\n",
    "final_output_file = 'hourly_averages_2.csv'\n",
    "\n",
    "calculate_hourly_average(input_file, output_directory, final_output_file)\n",
    "\n",
    "hourly_avg_final = pd.read_csv(final_output_file)\n",
    "print(hourly_avg_final.head())\n",
    "\n",
    "# סיבוכיות זמן - O(m) - טעינת הקובץ\n",
    "# O(m log h) - מיון לפי תאריך\n",
    "# O(m log d) - חלוקה לפי ימים\n",
    "# O(h_day log h) - חישוב ממוצעים לפי שעות\n",
    "# O(h_day) - שמירה לקובץ\n",
    "\n",
    "# סיבוכיות מקום - O(m)\n",
    "# כי טוען בתחילה את כל הקובץ\n",
    "# מעבד כל פעם רק O(h_day)\n",
    "# לכל יום את כמות השעות באותו יום\n",
    "# שומר בסוף קובץ בגודל O(h) כמות השעות"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ב.3\n",
    "# אם הנתונים מגיעים בזרימה\n",
    "\n",
    "# 1. נשמור במילון -  hashmap\n",
    "# סכום ומונה לכל שעה \n",
    "# 2. נעדכן סכום ומונה לשעה המתאימה\n",
    "# 3. נחשב ממוצע לפי חלוקה של הסכום למונה\n",
    "# 4. אפשר לעדכן בזמן אמת או במרווחים של זמן\n",
    "\n",
    "hourly_sums = {}\n",
    "hourly_counts = {}\n",
    "\n",
    "def update_hourly_averages(timestamp, value):\n",
    "    hour = timestamp.floor('H')\n",
    "    if hour not in hourly_sums:\n",
    "        hourly_sums[hour] = 0\n",
    "        hourly_counts[hour] = 0\n",
    "    hourly_sums[hour] += value\n",
    "    hourly_counts[hour] += 1\n",
    "    return hour, hourly_sums[hour] / hourly_counts[hour]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Timestamp  mean_value\n",
      "0 2025-06-01 00:00:00   50.562894\n",
      "1 2025-06-01 01:00:00   49.939803\n",
      "2 2025-06-01 02:00:00   49.457213\n",
      "3 2025-06-01 03:00:00   50.181573\n",
      "4 2025-06-01 04:00:00   48.611496\n",
      "Hourly averages saved to hourly_averages_3.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. ב.4\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(file_path):\n",
    "\n",
    "    if file_path.endswith('.csv'):\n",
    "        data = pd.read_csv(file_path)\n",
    "    elif file_path.endswith('.parquet'):\n",
    "        data = pd.read_parquet(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a CSV or Parquet file.\")\n",
    "    return data\n",
    "\n",
    "def calculate_hourly_averages(data):\n",
    "    \n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], dayfirst=True)\n",
    "    data['value'] = pd.to_numeric(data['value'], errors='coerce')\n",
    "    \n",
    "    data['hour'] = data['timestamp'].dt.floor('H')\n",
    "    \n",
    "    hourly_averages = data.groupby('hour')['value'].mean().reset_index()\n",
    "    hourly_averages.rename(columns={'hour': 'Timestamp', 'value': 'mean_value'}, inplace=True)\n",
    "    \n",
    "    return hourly_averages\n",
    "\n",
    "def save_results(data, output_file):\n",
    "    print(data.head())\n",
    "    data.to_csv(output_file, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"time_series_.parquet\"  \n",
    "    output_file = \"hourly_averages_3.csv\"\n",
    "    \n",
    "    data = load_data(input_file)\n",
    "    \n",
    "    hourly_averages = calculate_hourly_averages(data)\n",
    "    \n",
    "    save_results(hourly_averages, output_file)\n",
    "    print(f\"Hourly averages saved to {output_file}\")\n",
    "\n",
    "\n",
    "# יתרונות בשימוש ב- parquet:\n",
    "# 1. גודל קובץ קטן יותר - parquet דוחס את הנתונים בצורה טובה יותר.\n",
    "# 2. מהירות קריאה - parquet מיועד לקריאה מהירה יותר של נתונים גדולים.\n",
    "# 3. תמיכה בנתונים לא מסודרים - parquet תומך בנתונים לא מסודרים ובסכמות שונות.\n",
    "# 4. תמיכה ב- predicate pushdown - parquet מאפשר סינון נתונים בצורה טובה יותר.\n",
    "# 5. תמיכה ב- columnar storage - parquet שומר את הנתונים בצורה עמודתית, מה שמאפשר קריאה מהירה יותר של נתונים.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
